{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b17c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d950e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rag_queries(self, state: GraphState) -> GraphState:\n",
    "    email = state['current_email'].body\n",
    "    rag_querries = self.agents.generating_querry.invoke({email:email})\n",
    "    return {'rag_queries':rag_querries.querries}\n",
    "\n",
    "def retrive_from_rag(self, state: GraphState) -> GraphState:\n",
    "    final_ans = \"\"\n",
    "    for querry in state['rag_queries']:\n",
    "        ans = self.agents.generate_rag_answer.invoke(querry)\n",
    "        final_ans += querry + '\\n' + ans + '\\n\\n' \n",
    "    return {'retrived_document':final_ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49fb4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\miniconda3\\envs\\MCP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading & Chunking Docs...\n",
      "Creating vector embeddings...\n",
      "Test RAG chain...\n",
      "Question: What are your pricing options?\n",
      "Answer: Our pricing options are as follows:\n",
      "\n",
      "1. **Free Plan**: $0/month, which includes foundational model selection, template prompting, limited versioning, and training with sample data.\n",
      "2. **Pro Plan**: $49/month, which includes all free plan features, advanced tooling integration, prompt improvement system, enhanced versioning, and training with custom data.\n",
      "3. **Premium Plan**: $99/month, which includes all pro plan features, priority support, exclusive templates, custom tooling integration, and a dedicated account manager.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "RAG_SEARCH_PROMPT_TEMPLATE = \"\"\"\n",
    "Using the following pieces of retrieved context, answer the question comprehensively and concisely.\n",
    "Ensure your response fully addresses the question based on the given context.\n",
    "\n",
    "**IMPORTANT:**\n",
    "Just provide the answer and never mention or refer to having access to the external context or information in your answer.\n",
    "If you are unable to determine the answer from the provided context, state 'I don't know.'\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading & Chunking Docs...\")\n",
    "loader = TextLoader(\"agency.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "doc_chunks = doc_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Creating vector embeddings...\")\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "\n",
    "vectorstore = Chroma.from_documents(doc_chunks, embeddings, persist_directory=\"db\")\n",
    "\n",
    "# Semantic vector search\n",
    "vectorstore_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Test RAG chain\n",
    "print(\"Test RAG chain...\")\n",
    "prompt = ChatPromptTemplate.from_template(RAG_SEARCH_PROMPT_TEMPLATE)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1)\n",
    "llm = ChatNVIDIA(\n",
    "            model=\"meta/llama-3.1-70b-instruct\",  # choose any NVIDIA-supported model\n",
    "            )\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": vectorstore_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"What are your pricing options?\"\n",
    "result = rag_chain.invoke(query)\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Answer: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772d6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \n",
    "rag_querries = self.agents.generating_querry.invoke({email:email})\n",
    "print(rag_querries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
